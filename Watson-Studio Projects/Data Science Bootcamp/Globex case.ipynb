{
    "nbformat_minor": 1,
    "cells": [
        {
            "source": "# <center>Globex Case</center>\n<center><font size=\"3\">This is the analysis of the second case from the Data Science Challenges<br>\n    Done by: <b>Walid Ismail (wismail@eg.ibm.com)</b></font></center><br>",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "# 0. Introduction\nGlobex is a large industrial company with presence in six continents. They produce large industrial products such as ships, power plants, engines, etc. The CFO is examining profitability for numerous projects around the world, starting with projects beginning in 2010. They believe that using data and analytics can help identify projects that may be risky and result in a loss of profit. Globex has compiled data on all completed projects with a start date of 2010 or later. Globex needs the following:\n\n1. Determine the key predictors of a project\u2019s \u201cPROFIT_PLAN\u201d.\n2. Develop an actionable, innovative strategy for the client.\n3. Prepare a deliverable that is clear and concise, outlining each of your key recommendations.\n",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "# 1. Get the Watson Studio project token to use it to access the dataset",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='01cb8ab3-b48d-441d-8d33-edc6caae425d', project_access_token='p-d36f9300425b2c63be8cd6c20d68a07ef91ec977')\npc = project.project_context",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "# Import main needed libraries\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\nwarnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#%config InlineBackend.print_figure_kwargs={'bbox_inches':None}\n\nimport seaborn as sns\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "# 2. Load the data file",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "pd.set_option('display.width', 3000)\npd.set_option('display.max.columns', 500) # display up to 500 colums without \"...\" use horizontal scrolling instead\npd.set_option('display.max.rows', 50)\n\n# Fetch the Customer file\nmy_file = project.get_file(\"Globex_Data_Set.csv\")\n\n# Read the EXCEl data file from the object storage into a pandas DataFrame\nmy_file.seek(0)\nglobex_df = pd.read_csv(my_file)\nprint(\"\\nNumber of projects: {}\".format(len(globex_df)))\nglobex_df.head()",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "# 3. Exploratory Data Analysis\nLets now look at some exploratory information about our dataset and the variables in it to get a flovor of its different aspects. We will look at:<br>\n- Number of observations<br>\n- Number of columns<br>\n- Types of columns<br>\n- Does our dataset has empty cells or not?\n- Descriptive statistics for each variable",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "globex_df.shape # dimensions of data set",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "globex_df.isnull().sum() # check if any cells have missing values",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "globex_df.info() # check data type of each field",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "globex_df.describe() # view statisical measures of variables",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "globex_df.plot(kind='scatter', x='CTRCT_AMT_PLAN', y=['PROFIT_PLAN'])",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "# 4. Feature Engineering / Data Preparation\nFrom our data inspection we can observe the following about the variables / features / target:\n<ul>\n    <li>We have 27 attribuites in our dataset (26 possible features and one target)</li>\n    <li>Since Globex is seeking information on key predictors for 'PROFIT_PLAN', then our target variable for this analysis will be the variable <b>PROFIT_PLAN</b></li>\n    <li>All attributes are in numeric format.</li>\n    <li>We have <b>15</b> numeric variables that are continious in nature <b>(PROJ_DURN, CNTRY_DFT, FTE_INT, FTE_CONTR, CC, INSUR_BEFORE_AMT, INSUR_AFTER_AMT, INSUR_AMT, TAX_RT, CAP_UTIL, GNT_INVEST, LIQ_RSK, DIM_CONSTR, IPI, CTRCT_AMT_PLAN)</b>. These variables will require scaling before they are used in modeling</li>\n    <li><b>11</b> numeric variables <b>PROJ_CD_TFD, R1, R2, R3, R4, R5, R6, R7, CNTRY, REGION, CUST</b> that are numeric encoding of categorical variables so they  need to be tranformed to nominal variables of values (0,1)</li>\n</ul>",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "# 5. Feature Selection\nWe are going to use several methods to try and discover the features that will be strong predictors for customer churn. Specifically we will try the following techniques:\n\n- <b>Correlations</b>: this will help identify features pairs that are strongly correlated with each other so that we can remove one feature of them.\n- Also correlation will identify numeric features that are highly correlated with the target variable to select as predictors for modeling\n- <b>Box Plots</b> help us visualize the relationship between categorical variables and PROFIT_PLAN to see if there is variation in PROFIT_PLAN as a result of change in values for each of the categorical features. This should help us identify which categorical features qualify as a predictors for PROFIT_PLAN.\n- <b>SelectKBest method</b>: this is a scikit learn object that ranks a list of features according to their predictice impact by assigning a score to each feature",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "## 5.1 Correlations\nNow we check the Pearsom correlation between the continious features to see if any of the features pairs are highly correlated and as a result we can exclude one of the features of the pairs",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "#check for correlations between numeric features. Rule, if two features that are highly correlated one should be dropped\n# Does scaling make a difference in result ???? No, no need to scale features before running corr(), you will get the same scores. Try it yourself!\n\nnumeric_features = ['PROJ_DURN', 'CNTRY_DFT', 'FTE_INT', 'FTE_CONTR', 'CC', \n                    'INSUR_BEFORE_AMT', 'INSUR_AFTER_AMT', 'INSUR_AMT', \n                    'TAX_RT', 'CAP_UTIL', 'GNT_INVEST', 'LIQ_RSK', 'DIM_CONSTR', 'IPI', 'CTRCT_AMT_PLAN', 'PROFIT_PLAN']\n\n#plt.style.use('default')\nplt.figure(figsize=(12, 10))\nsns.heatmap(globex_df[numeric_features].corr(),  cmap=\"Blues\", annot=True, fmt=\".2f\")",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "The heat map above shows a maximum correlation of 0.67 between <b>employ</b> and <b>age</b>. All other features pairs have lower correlation. This correlation is not strong enough to warrant exclusion of any of the numeric features based on correlation alone.",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "## 5.2 Box plots to compare categorical variables against the target variables\nLet us check the 11 categorical features that we have in the dataset against <b>PROFIT_PLAN</b>. The categorical features are as follows:\n    <b>PROJ_CD_TFD, R1, R2, R3, R4, R5, R6, R7, CNTRY, REGION, CUST</b>",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "catgorical_features = ['PROJ_CD_TFD', 'R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'CNTRY', 'REGION', 'CUST']\ntarget_feature = 'PROFIT_PLAN'\n\nfont = {'weight' : 'normal',\n        'size'   : 16}\n\nplt.rc('font', **font)\nplt.rc('axes', titlesize=30)     # fontsize of the axes title\n\nfig, axs = plt.subplots(4, 3, sharex=False, sharey=True, figsize=(30,30))\nplt.subplots_adjust(hspace=0.4)\n\nfor i, feat in enumerate(catgorical_features):\n    ax = globex_df.boxplot(target_feature, by=catgorical_features[i], ax = axs[int(i/3), int(i%3)], showmeans=True)\n    ax.set_title('')\n    fig2 = ax.get_figure()\n    fig2.suptitle('')\n     \nfig.suptitle('PROFIT_PLAN grouped by different categroical variables', fontsize=24, y=0.95)\n\nfor i in range(11, 12):\n    fig.delaxes(axs[int(i/3), int(i%3)])\n   \nplt.show()",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "The cross tabulation above provides the following information about the relationship between each of the nominal features and churn:\n<ul>\n    <li>Each of the features <b>marital, gender, tollfree, multline, callid, confer</b> have the same distribution whether they are 0 or 1 on customer churn so we can conclude that they will be <b>weak</b> predictors for customer churn &#x274C;</li>\n    <li>Each of the features <b>equip, internet, ebill</b> have same distribution when they are 0 vs 1 on customer churn so we should take only one of them and use as a predictor. We shall select <b>equip</b></li>\n    <li>Each of the features <b>wireless, voice, pager</b> have same distribution when they are 0 vs 1 on customer churn so we should take only one of them and use as a predictor. We shall select <b>wireless</b></li>\n    <li><b>Callcard</b> havs an opposite distribution to all other nominal features where churn decreases as customer buy callcards. Accordingly, we shall select <b>callcard</b> as a predictor</li>       \n</ul>",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "## 5.5 SelectKBest Method\nSelectKBest is an object from Scikit Learn library that we can used for automatic feature selection. SelectKBest ranks all features based on their importance as predictors for the target variable using different statistical methods. We use it here as a validation of our selected features. As we can see from the result below, our selected features match the top 6 features ranked by SelectKBest.",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "# Feature Extraction with Univariate Statistical Tests\npd.set_option('display.expand_frame_repr', False)\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.feature_selection import f_classif\n\nall_features = ['PROJ_CD_TFD', 'R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', \n            'PROJ_DURN', 'CNTRY_DFT', 'CNTRY', 'REGION', 'FTE_INT', 'FTE_CONTR', 'CC', 'CUST', \n            'INSUR_BEFORE_AMT', 'INSUR_AFTER_AMT', 'INSUR_AMT', 'TAX_RT', 'CAP_UTIL', \n            'GNT_INVEST', 'LIQ_RSK', 'DIM_CONSTR', 'IPI', 'CTRCT_AMT_PLAN']\n\ncatgorical_features = ['PROJ_CD_TFD', 'R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', \n                        'CNTRY', 'REGION', 'CUST']\n\nnumeric_features = ['PROJ_DURN', 'CNTRY_DFT', 'FTE_INT', 'FTE_CONTR', 'CC', \n                    'INSUR_BEFORE_AMT', 'INSUR_AFTER_AMT', 'INSUR_AMT', \n                    'TAX_RT', 'CAP_UTIL', 'GNT_INVEST', 'LIQ_RSK', 'DIM_CONSTR', 'IPI', 'CTRCT_AMT_PLAN']\n\nmain_features = ['PROJ_CD_TFD', 'R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', \n            'PROJ_DURN', 'CNTRY_DFT', 'CNTRY', 'REGION', 'FTE_INT', 'FTE_CONTR', 'CC', 'CUST', \n            'TAX_RT', 'CAP_UTIL', \n            'GNT_INVEST', 'LIQ_RSK', 'DIM_CONSTR', 'IPI', 'CTRCT_AMT_PLAN']\n\ntarget = 'PROFIT_PLAN'\n\n# Does scaling make a difference in result ???? No, no need to scale features before running SelectKBest(), you will get the same scores. Try it yourself!\n\nfeatures_to_test = numeric_features\n\nX = globex_df[features_to_test]\nY = globex_df[target]\n\n# feature extraction\nselector = SelectKBest(score_func=f_regression, k=2)\nselector.fit(X, Y)\n\n#scores = -np.log10(selector.pvalues_)\nscores = selector.scores_\n\n# summarize scores\nnp.set_printoptions(precision=3, suppress=True)\npd.options.display.float_format = '{:.3f}'.format\nscored_features = pd.DataFrame({'Feature': features_to_test, 'Score' : scores})\nscored_features.sort_values(by = 'Score', ascending=False, inplace=True)\nscored_features = scored_features.reset_index(drop=True)\nscored_features.head(10)\n\n#features = fit.transform(X) # get the array of observations for top 5 features, NICE!!\n#print(features)",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "## 5.6 ANOVA Test",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "import statsmodels.api as sm\nfrom scipy import stats\n\nX = globex_df['R7']\ny = globex_df[target]\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "## 5.7 Chi-Squared Test",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "import scipy.stats as stats\ntab = pd.crosstab(globex_df['R4'], globex_df['R5'])\nprint(tab)\nprint(stats.chi2_contingency(tab))",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "## 5.6 Selected Features\nBased on the analysis above we have identified the following features as predictors for <b>PROFIT_PLAN</b> to be used in our modeling:<br>\n- 1\n- 2\n- 3\n- 4\n- 5\n- 6\n- 7\n- 8\n- 9",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "# 6. Modeling\nIn the section we will focus finding the best model in order to predict customer churn for WW Communications. Since <b>churn</b> is a nominal variable (0,1) then our problem is mainly a classification problem. We will try different classification models to see which one yields the highest training accuracy, then we will fine tune our most accurate model in order to arrive at the bet testing accuracy.",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "## 6.0 Transform Categorical Variables to Nominal Features",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "selected_features = ['PROJ_CD_TFD', 'R1', 'R2', 'R3', 'R5', 'R6', 'R7', 'CNTRY',\n                    'GNT_INVEST', 'CTRCT_AMT_PLAN'] + [target]\n\nglobex_df2 = globex_df[selected_features]\nglobex_df2 = pd.get_dummies(globex_df2, columns = ['PROJ_CD_TFD', 'R1', 'R2', 'R3', 'R5', 'R6', 'R7', 'CNTRY'])\nglobex_df2",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "## 6.1 Feature Scaling\nBefore modeling we need to do feature scaling in order to standardize our measurement scale for all numeric features",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler\n\ny = globex_df2[target]\nX = globex_df2.drop(target, axis=1)\n\n# ALWAYS SPLIT FIRST BEFORE SCALING TO AVOID PREDICTOR LEAKAGE !!!!!\n\n#Train Test Split, reserve test subset for finally accuracy and other metrics calculations\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\nscaler = MinMaxScaler(feature_range = (0, 1))\nscaler.fit(X_train) # fit the scaler on training data then apply to training and test data\nX_train = scaler.transform(X_train)\nX_test =  scaler.transform(X_test)\nprint('Done')",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "## 6.2 Modeling (using different models)\nNow lets try three different classification models and see which one yields the best training accuracy. We will try the following three models:\n- Logistic Regression\n- Support Vector Classifier\n- Random Forest Classifier",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "from math import sqrt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# using specific random state to make sure we always get same accuracy result if we execute this cell multiple times or on multiple machines\n\nlm = LinearRegression()\nscores = cross_val_score(lm, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\nprint(\"Linear regression RMSE is: {:,}\".format(sqrt(-scores.mean())))\n\nsvr = svm.SVR()\nscores = cross_val_score(svr, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\nprint(\"SVM RMSE is: {:,}\".format(sqrt(-scores.mean())))\n\ndcr = DecisionTreeRegressor()\nscores = cross_val_score(dcr, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\nprint(\"Descision Tree RMSE is: {:,}\".format(sqrt(-scores.mean())))\n\nknnr = KNeighborsRegressor(n_neighbors=30)\nscores = cross_val_score(dcr, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\nprint(\"KNeighborsRegressor RMSE is: {:,}\".format(sqrt(-scores.mean())))\n\npoly = PolynomialFeatures(2)\nX_transform = poly.fit_transform(X_train)\nscores = cross_val_score(lm, X_transform, y_train, cv=5, scoring='neg_mean_squared_error')\nprint(\"Ploynomial RMSE is: {:,}\".format(sqrt(-scores.mean())))",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "## 6.3 Fine tuning the selected model hyperparameters\nSince Logistic Regression yielded the best training accuracy then we will select that model as our estimator of churn and now we need to identify the best hyperparameters to initialize the model in order to yield the highest possible testing accuracy",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "from sklearn.model_selection import GridSearchCV\n#from sklearn.model_selection import RandomizedSearchCV\nimport numpy as np\n\nC_options = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\nsolver_options = ['newton-cg', 'lbfgs', 'liblinear', 'sag']\nclass_weight_options = [None, 'balanced']\nparam_grid = dict(C = C_options, solver = solver_options, class_weight = class_weight_options)\n\nclf = GridSearchCV(LogisticRegression(), param_grid)\n\nclf.fit(X_train, y_train)\n\nbest_model = clf.best_estimator_\nbest_model",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "# 7. Model Evaluation\nIn this section we check the tuned model testing accuracy. We also take a look at the relative importance of the model features in predicting customer churn. We conclude with an overview of the confusion matrix",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "## 7.1 Testing accuracy",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "y_pred = best_model.predict(X_test)\n\nfrom sklearn.metrics import classification_report\n\nprint('\\nAccuracy of tuned model on test set: {0:.2f}\\n'.format(best_model.score(X_test, y_test)))\n\nprint(classification_report(y_test, y_pred))",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "## 7.2 Displaying the modeling predictors sorted by coefficent\nIn this section we take a look at the influence of the model features on customer churn.<br>\n- Features with +ve coefficient drive customer churn (i.e. churn = 1)\n- Features with -ve coefficient drive customer retention (i.e churn = 0)",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "import pandas as pd\n\n# model is the trained model\nimportances = best_model.coef_[0]\nprint(importances)\n\n# Extract the feature importances into a dataframe\nfeature_results = pd.DataFrame({'feature': features, \n                                'importance': importances})\n\n# Show the top 10 most important\nfeature_results = feature_results.sort_values('importance', ascending = False).reset_index(drop=True)\n\nfeature_results.head(10)",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "#plt.style.use('fivethirtyeight')\n#plt.style.use('default')\n# Plot the most important features in a horizontal bar chart\nfeature_results.plot(x = 'feature', y = 'importance', edgecolor = 'k', kind='barh', color = 'blue', figsize=(6, 3))\nplt.xlabel('Relative Importance', size = 12)\nplt.ylabel('')\nplt.title('Feature Importances', size = 12)\nplt.grid(False)\nplt.show()",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "## 7.3 Confsuion Matrix\nFinally we take a look the resulting confusion matrix and evaluate our model performance in predicting customer churn vs retention. Specifically we will exanme the following metrics:\n\nPrecision (the positive predictive value) = TP / (TP+FP)\n<br>\nSpecificity (the true negative rate) = TN / (FP+TN)\n<br>\nAccuracy (the fraction of the total sample that is correctly identified) = (TP+TN) / (TP+TN+FP+FN)\n",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "from sklearn.metrics import classification_report, confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    plt.grid(False)\n\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_pred, [1, 0])\nnp.set_printoptions(precision=2)\n\ntp, fn, fp, tn = cnf_matrix.ravel()\n\naccuracy = (tn+tp) / (tn + fp + fn + tp)\nprecision = tp / (tp+fp)\nspecifity = tn / (fp+tn)\n\nprint(\"Accuracy = {0:.2f}\".format(accuracy))\nprint(\"Precision = {0:.2f}\".format(precision))\nprint(\"Specifity = {0:.2f}\".format(specifity))\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(4,4))\nplot_confusion_matrix(cnf_matrix, classes=['churn=1','churn=0'],  title='Confusion matrix')",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "# 8. Conclusions and Recommendations\nBased on our analysis we provide the following conclusions and recommendations for the future:\n- The strongest three predictors affecting consumer retention (churn = 0) are as follows:\n    - tenure\n    - address\n    - employ\n- The strongest three predictors affecting consumer churn are as follows:\n    - income\n    - equip\n    - age\n- Our model is better at predicting retention (churn = 0) than churn (churn = 1). This is due to the dataset containing more observations with churn=0 than churn=1. Our model should become better at predicting churn if the dataset is updated with more observations for +ve churn customers\n- Customers in certain address code (range 0-11) have a high churn percentage. This could be due to network issues in theses areas. Futher investigation is advised\n- Customers in low income range (0-47) are more likely to leave WW. Further analysis is advised on price plans for these customers\n- Customers who have equip = 1 are more likely to leave WW. Futher analysis advised to uncover reasons for this\n- Customers with high years of education and employment are more loyal. Should proivde more incentives to attract these types of customers",
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": "",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "source": "",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        }
    ],
    "nbformat": 4,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5",
            "name": "python3",
            "language": "python"
        },
        "language_info": {
            "mimetype": "text/x-python",
            "nbconvert_exporter": "python",
            "version": "3.5.5",
            "name": "python",
            "pygments_lexer": "ipython3",
            "file_extension": ".py",
            "codemirror_mode": {
                "version": 3,
                "name": "ipython"
            }
        }
    }
}